{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Engineering, and Model Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import  randint\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target (what are we trying to predict)\n",
    "target = 'pwt_500hpa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lists of coordinates and time ranges (pulled from Prepare_AI_Ready_Data.py)\n",
    "coords = [[180,240,45,65],[130,250,20,75]]\n",
    "times = [['1940-01-01','2023-12-31']]  # Ensure the time range is valid\n",
    "\n",
    "# set PC option - seperate or combined\n",
    "#PC_option = 'combined'\n",
    "PC_option = 'seperate'\n",
    "\n",
    "# select which of the list I want to load\n",
    "coords_num = 1\n",
    "times_num = 0\n",
    "\n",
    "# pull the correct coordinate and time (as set above)\n",
    "c = coords[coords_num]\n",
    "t = times[times_num]\n",
    "\n",
    "# read in PCs\n",
    "if PC_option == 'combined':\n",
    "    raw_data = pd.read_csv(f'../data/dimensionality_reduction/principal_components_combined_{c[0]}-{c[1]}_{c[2]}-{c[3]}_{t[0][:4]}-{t[1][:4]}_target.csv')\n",
    "elif PC_option == 'seperate':\n",
    "    raw_data = pd.read_csv(f'../data/dimensionality_reduction/principal_components_seperate_{c[0]}-{c[1]}_{c[2]}-{c[3]}_{t[0][:4]}-{t[1][:4]}_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns that are target, contain \"PC\", or contain \"month\"\n",
    "columns_to_keep = [col for col in raw_data.columns if target in col or \"PC\" in col or \"month\" in col]\n",
    "data = raw_data[columns_to_keep]\n",
    "\n",
    "# normalize all float columns to vary between 0 and 1, but ignore boolean columns\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'float64':\n",
    "        data[col] = (data[col] - data[col].min()) / (data[col].max() - data[col].min())\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "While the preformance of all of the models in AutoML wasn't fantstic, the linear regression approach was near the top of the list. And, this is the simplest approach (hyperparameter tunning doesn't even apply, as there aren't that many parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into independent and dependent variables\n",
    "#X = data[['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8','PC9','PC10', 'month']]\n",
    "#Y = data['pwt_500hpa']\n",
    "Y = data[target]\n",
    "X = data.drop(columns=[target])\n",
    "\n",
    "# Splitting the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "t1 = time.perf_counter()\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "t2 = time.perf_counter()\n",
    "t_lr_onerun = t2-t1\n",
    "\n",
    "# Calculating the test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"mean square error {mse} and R2 {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - k-fold cross validation\n",
    "\n",
    "Here we'll use k-fold cross validation to robustly estimate the accuracy of this model. Our runtime is fast enough we could use something even more accruate, like leave one out cross validation. However, this takes too long further down, and I want to stay consistent with my assesment approach accross models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's impliment a k-fold cross validation of the linear regression model\n",
    "\n",
    "# Start timer\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# Initialize KFold\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(\n",
    "    regressor,\n",
    "    X,\n",
    "    Y,\n",
    "    cv=kfold,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# End timer\n",
    "t2 = time.perf_counter()\n",
    "t_lr_kfold = t2 - t1\n",
    "\n",
    "# calculate mae, mse, and rmse\n",
    "mae = -cross_val_score(regressor, X, Y, cv=kfold, scoring='neg_mean_absolute_error').mean()\n",
    "mse = -cross_val_score(regressor, X, Y, cv=kfold, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'model': 'LinearRegression',\n",
    "    'mean_r2': scores.mean(),\n",
    "    'std_r2': scores.std(),\n",
    "    'mean_absolute_error': mae,\n",
    "    'mean_squared_error': mse,\n",
    "    'root_mean_squared_error': np.sqrt(mse),\n",
    "    '1 Run Time': t_lr_onerun,\n",
    "    'K-Fold Time': t_lr_kfold\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for key, value in results.items():\n",
    "    print(key + ':', value)\n",
    "\n",
    "# Convert results to DataFrame and append to results_df\n",
    "results_df = pd.DataFrame(results, index=[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Learning Curve\n",
    "\n",
    "We can use this to get a sense for how the model training responds to different training sizes. There's a lot of noise here, pointing to the model being very sensitive to the training subset. Absolute preformance is also, of course, quite low. Even the training score is low, even at higher sample numbers. This might towards difficultly fitting with a purely linear approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curve data\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    regressor, X, Y, cv=5, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"blue\")\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\", color=\"orange\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"orange\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the preformance of the linear regression above is pretty weak. Now let's try modifying the 'month' column to give this a chance - we'll make it months from the hotest month time of year (July/August). July/August will be a '0', June/Sepetember a 1, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this was a big improvement! We jumped from r^2 <0.1 to r^2 ~ 0.65. This points to the importance of having a physically interpretable month. However, this r^2 value is still significantly worse than what I was achieving in the auto_ml notebook. Rather than further develop/test the linear model, I'm going to switch to some more complex approaches to see what I can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "Here I train a random forest regression. While this wasn't the top-preforming approach in the AutoML methods, it is a commonly used method in the geosciences. The interpretability of this model makes it worthwhile to consider for our application. Likewise, I'm keen to explore an approach which is able to handle non-linear relationships.\n",
    "\n",
    "Note that I've already preformed onehot encoding on the month column before starting here, so the month data is included as a set of 12 boolean variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - First Pass\n",
    "\n",
    "Here I'll just do a simple first pass, giving us a sense of the accuracy and runtime of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are the values we want to predict\n",
    "labels = np.array(data[target])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = data.drop(columns=[target])\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# split the data into test/training sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# establish the model baseline. We'll make averages over the time the baseline prediction\n",
    "baseline_preds = np.mean(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# initalize the model\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "# save training time\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Training time: {t2-t1} seconds\")\n",
    "t_rf = t2-t1\n",
    "\n",
    "# print basic metrics about the model\n",
    "predictions = rf.predict(test_features)\n",
    "errors = abs(predictions - test_labels)\n",
    "mae = round(np.mean(errors), 2)\n",
    "mse = mean_squared_error(test_labels, predictions)\n",
    "r2 = r2_score(test_labels, predictions)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('R²:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Hyperparameter Training\n",
    "\n",
    "We're off to a start - we've learned this model is fast, but the time is not negligibly so at 3s (a ton of folds could add up). Preformance seems fairly weak, but we may be able to improve slightly with better taining\n",
    "\n",
    "The number of estimators (trees in the forstest), four options between 100 and 1000. More should be better, but with diminishing yeild\n",
    "The maximum depth of the tree, from 10 to 30, plus a 'None' option. Some risk of overfitting as we get too deep.\n",
    "The Maximum number of features considered at each split. I consider a range of approaches, including considering everything (1.0) to 1 (1), along with 'sqrt' and 'log2' which are standard in-between approaches.\n",
    "The minimum samples  to split an internal node. I used chatGPT to guide me to a reasonable range here.\n",
    "The minimum samples in a leaf node. \n",
    "I also toggle bootstrapping on/off, setting if I draw samples with replacement while training each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "# Define a parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'max_features': ['sqrt','log2',1,1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the random forest regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "rf_random.fit(features, labels)\n",
    "\n",
    "# store time\n",
    "t2 = time.perf_counter()\n",
    "t_rf_hyperparam = t2-t1\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", rf_random.best_params_)\n",
    "print(\"Best CV Score:\", rf_random.best_score_)\n",
    "print(\"R^2 of the best estimator:\", rf_random.best_estimator_.score(test_features, test_labels))\n",
    "\n",
    "# Retrieve the best model\n",
    "best_rf_model = rf_random.best_estimator_\n",
    "\n",
    "\n",
    "# store parameters in a way they can be used in k-fold cross validation below.\n",
    "n_estimators = best_rf_model.get_params()['n_estimators']\n",
    "max_depth = best_rf_model.get_params()['max_depth']\n",
    "max_features = best_rf_model.get_params()['max_features']\n",
    "min_samples_split = best_rf_model.get_params()['min_samples_split']\n",
    "min_samples_leaf = best_rf_model.get_params()['min_samples_leaf']\n",
    "bootstrap = best_rf_model.get_params()['bootstrap']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - K-fold cross validation\n",
    "\n",
    "Now that we've assessed a single model, let's do a more comprehensive analysis.\n",
    "\n",
    "My gut here is to do a leave out cross validation. We have a relatively small dataset, and might be able to get away with this computationally intensive approach. However, if we take the runtime (~3s) and the number of samples (~640) we quickly realize this puts us at a total runtime of over 30 minutes. Whoops - that's too much for our purposes.\n",
    "\n",
    "Instead, let's use a K-fold cross validation. We'll get many of the same benefits, including a very (but slightly less) accurate picture of our predictive accuracy, but with less computational effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(\n",
    "    best_rf_model,\n",
    "    features,\n",
    "    labels,\n",
    "    cv=kfold,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "t_rf_kfold = t2-t1\n",
    "\n",
    "# Add Random Forest results to the results_df\n",
    "rf_results = {\n",
    "    'model': 'RandomForestRegressor',\n",
    "    'mean_r2': scores.mean(),\n",
    "    'std_r2': scores.std(),\n",
    "    'mean_absolute_error': mae,\n",
    "    'mean_squared_error': mse,\n",
    "    'root_mean_squared_error': np.sqrt(mse),\n",
    "    '1 Run Time': t_rf,\n",
    "    'K-Fold Time': t_rf_kfold,\n",
    "    'Hyperparameter Time': t_rf_hyperparam\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(rf_results, index=[0])\n",
    "\n",
    "# combine with existing results_df\n",
    "results_df = pd.concat([results_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Learning Curve\n",
    "\n",
    "There's quite a large gap here between the training score (impresive) and the cross validation score (poor). We don't seem to be improving much with imcreasing sample size at this scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curve data for the random forest model\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    rf, features, labels, cv=5, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"blue\")\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\", color=\"orange\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"orange\")\n",
    "plt.title(\"Learning Curve for Random Forest\")\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Interpretibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_list,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n",
    "plt.title('Feature Importance for Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber Regressor\n",
    "\n",
    "This was the top-preforming model in my autoML approach. This may be due to the huber regressor's robust handling of outliers, which is a particular strength of the model. It is also fairly strightforward, with good ability to handle complex data.\n",
    "\n",
    "I'll start with a simple implimentation of the approach, to get a sense of runtime and rough preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial Model Run\n",
    "t1 = time.perf_counter()\n",
    "huber = HuberRegressor()\n",
    "huber.fit(train_features, train_labels)\n",
    "y_pred = huber.predict(test_features)\n",
    "t2 = time.perf_counter()\n",
    "t_huber_onerun = t2-t1\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(test_labels, y_pred)\n",
    "mae = mean_absolute_error(test_labels, y_pred)\n",
    "r2 = r2_score(test_labels, y_pred)\n",
    "print(f\"Initial Model - MSE: {mse}, MAE: {mae}, R2: {r2}, Training Time: {t2 - t1} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "param_distributions = {\n",
    "    'epsilon': [1.1, 1.2, 1.3, 1.35, 1.4, 1.5],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'max_iter': [100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "huber_random = RandomizedSearchCV(\n",
    "    estimator=HuberRegressor(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "huber_random.fit(train_features, train_labels)\n",
    "t2 = time.perf_counter()\n",
    "t_huber_hyperparam = t2-t1\n",
    "\n",
    "# Best model and metrics\n",
    "best_huber_model = huber_random.best_estimator_\n",
    "best_params = huber_random.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"R^2 of the best estimator: {best_huber_model.score(test_features, test_labels)}\")\n",
    "print(f\"Hyperparameter Tuning Time: {t2 - t1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-Fold Analysis\n",
    "t1 = time.perf_counter()\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(\n",
    "    best_huber_model,\n",
    "    features,\n",
    "    labels,\n",
    "    cv=kfold,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "t2 = time.perf_counter()\n",
    "t_huber_kfold = t2-t1\n",
    "\n",
    "# calculate mae, mse\n",
    "y_pred = best_huber_model.predict(test_features)\n",
    "mse = mean_squared_error(test_labels, y_pred)\n",
    "mae = mean_absolute_error(test_labels, y_pred)\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'model': 'HuberRegressor',\n",
    "    'best_params': best_params,\n",
    "    'mean_r2': scores.mean(),\n",
    "    'std_r2': scores.std(),\n",
    "    'mean_absolute_error': mae,\n",
    "    'mean_squared_error': mse,\n",
    "    'root_mean_squared_error': np.sqrt(mse),\n",
    "    '1 Run Time': t_huber_onerun,\n",
    "    'Hyperparameter Time': t_huber_hyperparam,\n",
    "    'K-Fold Time': t_huber_kfold\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for key, value in results.items():\n",
    "    print(key + ':', value)\n",
    "\n",
    "df = pd.DataFrame(results, index=[0])\n",
    "\n",
    "# combine with existing results_df\n",
    "results_df = pd.concat([results_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Regressor - Learning Curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curve data for the Huber Regressor\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_huber_model, features, labels, cv=5, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"blue\")\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\", color=\"orange\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"orange\")\n",
    "plt.title(\"Learning Curve for Huber Regressor\")\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Regressor - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the feature importance of the huber regressor\n",
    "# Visualize the feature importance of the Huber Regressor\n",
    "# Since Huber Regressor does not provide feature importances directly,\n",
    "# we can use the coefficients as a proxy for feature importance.\n",
    "\n",
    "# Extract feature importance (coefficients)\n",
    "feature_importances = best_huber_model.coef_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_list,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n",
    "plt.title('Feature Importance for Huber Regressor')\n",
    "plt.xlabel('Coefficient Value (Importance)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and append to results_df\n",
    "rf_results_df = pd.DataFrame(rf_results, index=[0])\n",
    "results_df = pd.concat([results_df, rf_results_df], ignore_index=True)\n",
    "\n",
    "# Extract relevant data for plotting\n",
    "models = results_df['model']\n",
    "r2_scores = results_df['mean_r2']\n",
    "times = results_df['K-Fold Time']\n",
    "#times = results_df['Hyperparameter Time'].fillna(0) + results_df['K-Fold Time']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(times, r2_scores, color='blue', s=100, alpha=0.7)\n",
    "\n",
    "# Annotate each point with the model name\n",
    "for i, model in enumerate(models):\n",
    "    plt.text(times.iloc[i], r2_scores.iloc[i], model, fontsize=13, ha='left')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Model Comparison: R² vs Time', fontsize=14)\n",
    "plt.xlabel('K-fold Time (s)')\n",
    "#plt.xlabel('Time (Hyperparameter + K-Fold) [seconds]', fontsize=12)\n",
    "plt.ylabel('R² Score', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Regressor, applied to Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target\n",
    "target = 'temperature_500hpa'\n",
    "\n",
    "# Keep only columns that are target, contain \"PC\", or contain \"month\"\n",
    "columns_to_keep = [col for col in raw_data.columns if target in col or \"PC\" in col or \"month\" in col]\n",
    "data_temp = raw_data[columns_to_keep]\n",
    "\n",
    "# normalize all float columns to vary between 0 and 1, but ignore boolean columns\n",
    "for col in data_temp.columns:\n",
    "    if data_temp[col].dtype == 'float64':\n",
    "        data_temp[col] = (data_temp[col] - data_temp[col].min()) / (data_temp[col].max() - data_temp[col].min())\n",
    "\n",
    "# break out features and labels\n",
    "\n",
    "labels = np.array(data_temp[target])\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_distributions = {\n",
    "    'epsilon': [1.1, 1.2, 1.3, 1.35, 1.4, 1.5],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'max_iter': [100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "huber_random = RandomizedSearchCV(\n",
    "    estimator=HuberRegressor(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "huber_random.fit(train_features, train_labels)\n",
    "t2 = time.perf_counter()\n",
    "t_huber_hyperparam = t2-t1\n",
    "\n",
    "# Best model and metrics\n",
    "best_huber_model_temp = huber_random.best_estimator_\n",
    "best_params = huber_random.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"R^2 of the best estimator: {best_huber_model_temp.score(test_features, test_labels)}\")\n",
    "print(f\"Hyperparameter Tuning Time: {t2 - t1} seconds\")\n",
    "\n",
    "# K-Fold Analysis\n",
    "t1 = time.perf_counter()\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(\n",
    "    best_huber_model_temp,\n",
    "    features,\n",
    "    labels,\n",
    "    cv=kfold,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "t2 = time.perf_counter()\n",
    "t_huber_kfold = t2-t1\n",
    "\n",
    "# calculate mae, mse\n",
    "y_pred = best_huber_model_temp.predict(test_features)\n",
    "mse = mean_squared_error(test_labels, y_pred)\n",
    "mae = mean_absolute_error(test_labels, y_pred)\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'model': 'HuberRegressor-temperature_500hpa',\n",
    "    'best_params': best_params,\n",
    "    'mean_r2': scores.mean(),\n",
    "    'std_r2': scores.std(),\n",
    "    'mean_absolute_error': mae,\n",
    "    'mean_squared_error': mse,\n",
    "    'root_mean_squared_error': np.sqrt(mse),\n",
    "    'Hyperparameter Time': t_huber_hyperparam,\n",
    "    'K-Fold Time': t_huber_kfold\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for key, value in results.items():\n",
    "    print(key + ':', value)\n",
    "\n",
    "df = pd.DataFrame(results, index=[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curve data for the Huber Regressor\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_huber_model_temp, features, labels, cv=5, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"blue\")\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\", color=\"orange\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"orange\")\n",
    "plt.title(\"Learning Curve for Huber Regressor\")\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ess469_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
